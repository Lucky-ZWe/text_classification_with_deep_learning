{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_type_content_mark = '|'\n",
    "base_data_dir = \"../../data/sogou_resource/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"转换指定目录下文件的编码为utf-8\"\"\"\n",
    "def convert(filename,from_encode=\"GBK\",to_encode=\"UTF-8\"):\n",
    "    try:\n",
    "#         print \"convert \"+ filename\n",
    "        content = open(filename,'rb').read()\n",
    "#         print \"content:\"+content\n",
    "        new_content = content.decode(from_encode).encode(to_encode)\n",
    "#         print \"new_content:\"+new_content\n",
    "        open(filename,\"w\").write(new_content)\n",
    "        print \"done\"\n",
    "    except:\n",
    "        print \"error\"\n",
    "\n",
    "def explore(dir):\n",
    "    for root,dirs,files in os.walk(dir):\n",
    "        for file in files:\n",
    "            path = os.path.join(root,file)\n",
    "            convert(path)\n",
    "\n",
    "def do_convert(path):\n",
    "    if os.path.isfile(path):\n",
    "        convert(path)\n",
    "    if os.path.isdir(path):\n",
    "        explore(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/sogou_resource/\"\n",
    "do_convert(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"使用jieba分词进行中文分词\"\"\"\n",
    "import jieba\n",
    "import zhon\n",
    "\n",
    "separated_word_file_dir = \"word_separated\"\n",
    "# 清华新闻语料库\n",
    "types = [\"体育\",\"娱乐\",\"家居\",\"彩票\",\"房产\",\"教育\",\"时尚\",\"时政\",\"星座\",\"游戏\",\"社会\",\"科技\",\"股票\",\"财经\"]\n",
    "# types = [\"体育\"]\n",
    "\n",
    "\n",
    "# 搜狗实验室新闻语料类别\n",
    "# types = [\"体育\",\"健康\",\"军事\",\"招聘\",\"文化\",\"教育\",\"财经\",\"旅游\",\"互联网\"]\n",
    "\n",
    "def ch_and_en_word_extraction(content_raw):\n",
    "    \"\"\"抽取中文和英文\"\"\"\n",
    "    pattern = re.compile(u\"([\\u4e00-\\u9fa5a-zA-Z0-9]+)\")\n",
    "    re_data = pattern.findall(content_raw)\n",
    "    clean_content = ' '.join(re_data)\n",
    "    return clean_content\n",
    "\n",
    "def clean_str(s):\n",
    "    s = s.strip() # 前后的空格\n",
    "    s = s.strip('\\n') #换行符\n",
    "    s = re.sub(\"[ \\t\\n\\r]*\",'',s) # tab, newline, return\n",
    "    s = re.sub(\"<(\\S*?)[^>]*>.*?</\\1>|<.*? />\",'',s) # html标签\n",
    "    s = re.sub(\"&nbsp+|&lt+|&gt+\",'',s) # html中的空格符号,大于，小于\n",
    "    s = re.sub(\"[a-zA-z]+://[^\\s]*\",'',s) # URL\n",
    "    s = re.sub(r'([\\w-]+(\\.[\\w-]+)*@[\\w-]+(\\.[\\w-]+)+)','',s) # email\n",
    "    # 标点符号,需要先转utf-8，否则符号匹配不成功\n",
    "    s = re.sub(ur\"([%s])+\" %zhon.hanzi.punctuation,\" \",s.decode('utf-8'))\n",
    "    # 抽取中文和英文\n",
    "    s = ch_and_en_word_extraction(s)\n",
    "    return s\n",
    "\n",
    "def separate_words(infile,outfile):\n",
    "    try:\n",
    "        outf = open(outfile,'w')\n",
    "        inf = open(infile,'r')\n",
    "\n",
    "        space = ' '\n",
    "#         print 'separate '+infile\n",
    "        for line in inf.readlines():\n",
    "#             line = clean_str(line)\n",
    "            # 除空行\n",
    "            if not len(line.strip()):\n",
    "                continue\n",
    "            seg_list = jieba.cut(line)\n",
    "            \"\"\"此处需要循环每个单词编码为utf-8，jieba.cut将结果转为了unicode编码，\n",
    "            直接write(space.join(seg_list))会报编码错误\"\"\"\n",
    "            for word in seg_list:\n",
    "                try:\n",
    "                    word = word.encode('UTF-8')\n",
    "                except:\n",
    "                    continue\n",
    "                outf.write(word)\n",
    "                outf.write(' ')\n",
    "        outf.write('\\n')\n",
    "        # close file stream\n",
    "        outf.close()\n",
    "        inf.close()\n",
    "    except:\n",
    "        pass\n",
    "#         print \"error occured when write to \"+outfile\n",
    "\n",
    "\n",
    "\n",
    "def is_target_dir(path):\n",
    "    if os.path.dirname(path).split(\"/\")[-1] in types and not re.match(\".DS_Store\",os.path.basename(path)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def explore(dir):\n",
    "    for root,dirs,files in os.walk(dir):\n",
    "        for file in files:\n",
    "            path = os.path.join(root,file)\n",
    "            if is_target_dir(path):\n",
    "                child_dir = os.path.join(root,separated_word_file_dir)\n",
    "                if not os.path.exists(child_dir):\n",
    "                    os.mkdir(child_dir)\n",
    "                    print \"make dir: \"+child_dir\n",
    "                separate_words(path,os.path.join(child_dir,file))\n",
    "                \n",
    "\n",
    "def do_batch_separate(path):\n",
    "    if os.path.isfile(path) and is_target_dir(path):\n",
    "        separate_words(path,os.path.join(root,separated_word_file_dir,path))\n",
    "    if os.path.isdir(path):\n",
    "        explore(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin time: 2017-11-11 00:32:09.642573\n",
      "make dir: ../../data/THUCNews/体育/word_separated\n",
      "make dir: ../../data/THUCNews/娱乐/word_separated\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "original_dir = \"../../data/THUCNews/\"\n",
    "now = datetime.datetime.now()\n",
    "print \"begin time:\",now\n",
    "begin_time = time.time()\n",
    "do_batch_separate(original_dir)\n",
    "end_time = time.time()\n",
    "now = datetime.datetime.now()\n",
    "print \"end time:\",now\n",
    "print \"time used:\"+str(end_time-begin_time)+\"秒\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"将所有语料，整合成csv类型文件，文件格式：type,content\"\"\"\n",
    "def combine_file(file,outfile):\n",
    "    # the type of file ，file示例：xxx/互联网／xxx/xxx.txt\n",
    "    label = os.path.dirname(file).split('/')[-2]\n",
    "    content = open(file).read()\n",
    "#     print \"content:\"+content\n",
    "#     print \"len:\",len(content)\n",
    "    if len(content)>1: #排除前面步骤中写文件时，内容为只写入一个空格的情况\n",
    "        new_content = label+\",\"+content\n",
    "#         print \"new_content:\\n \" + new_content\n",
    "        open(outfile,\"a\").write(new_content)\n",
    "\n",
    "def do_combine(dir,outfile):\n",
    "    print \"deal with dir: \"+ dir\n",
    "    for root,dirs,files in os.walk(dir):\n",
    "        for file in files:\n",
    "            match = re.match(r'\\d+\\.txt',file)\n",
    "            if match:\n",
    "                path = os.path.join(root,file)\n",
    "                print \"combine \"+ path\n",
    "                combine_file(path,outfile)\n",
    "                \n",
    "def create_csv_file(dir,filename):\n",
    "    csv_title = \"type,content\\n\"\n",
    "    filepath = os.path.join(dir,filename+'.csv')\n",
    "    open(filepath,'w').write(csv_title)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../../data/THUCNews/\"\n",
    "\"\"\"创建处理后的数据集的目录\"\"\"\n",
    "dataset_dir = os.path.join(base_dir,\"dataset\")\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.mkdir(dataset_dir)\n",
    "    \n",
    "\"\"\"创建每个type目录对应的csv文件，并将一个type目录下的文件写到同一个对应的csv文件\"\"\"\n",
    "# 清华新闻语料库\n",
    "type_name_list = [\"体育\",\"娱乐\",\"家居\",\"彩票\",\"房产\",\"教育\",\"时尚\",\"时政\",\"星座\",\"游戏\",\"社会\",\"科技\",\"股票\",\"财经\"]\n",
    "\n",
    "# 搜狗新闻语料库\n",
    "# type_name_list = [\"体育\",\"健康\",\"军事\",\"招聘\",\"文化\",\"教育\",\"财经\",\"旅游\",\"互联网\"]\n",
    "\n",
    "for name in type_name_list:\n",
    "    path = create_csv_file(dataset_dir,name)\n",
    "    print \"going to combine file to  \" + path\n",
    "    do_combine(os.path.join(base_dir,name,\"word_separated\"),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"随机采样每个类别的约20%作为测试集,80%作为训练集\"\"\"\n",
    "import random\n",
    "    \n",
    "def extract_test_and_train_set(filepath,train_file,test_file):\n",
    "    try:\n",
    "        test_f = open(test_file,'a')\n",
    "        train_f = open(train_file,'a')\n",
    "        try:\n",
    "            with open(filepath) as f:\n",
    "                is_title_line = True\n",
    "                for line in f.readlines():\n",
    "                    if is_title_line:\n",
    "                        is_title_line = False\n",
    "                        continue\n",
    "                    if not len(line):\n",
    "                        continue\n",
    "                    if random.random() <= 0.2:\n",
    "                        test_f.write(line)\n",
    "                    else:\n",
    "                        train_f.write(line)\n",
    "        except:\n",
    "            print \"IO ERROR\"\n",
    "        finally:\n",
    "            test_f.close()\n",
    "            train_f.close()\n",
    "    except:\n",
    "        print \"can not open file\"\n",
    "\n",
    "def do_extract(source_dir,train_f,test_f):\n",
    "    for root,dirs,files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if re.match(\"test|train\\.csv\",file) or not re.match(\".*\\.csv\",file):\n",
    "                continue\n",
    "            path = os.path.join(root,file)\n",
    "            print \"extract file: \"+ path\n",
    "            extract_test_and_train_set(path,train_f,test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create file: ../../data/sogou_resource/dataset/train.csv\n",
      "create file:../../data/sogou_resource/dataset/test.csv\n",
      "extract file: ../../data/sogou_resource/dataset/互联网.csv\n",
      "extract file: ../../data/sogou_resource/dataset/体育.csv\n",
      "extract file: ../../data/sogou_resource/dataset/健康.csv\n",
      "extract file: ../../data/sogou_resource/dataset/军事.csv\n",
      "extract file: ../../data/sogou_resource/dataset/招聘.csv\n",
      "extract file: ../../data/sogou_resource/dataset/教育.csv\n",
      "extract file: ../../data/sogou_resource/dataset/文化.csv\n",
      "extract file: ../../data/sogou_resource/dataset/旅游.csv\n",
      "extract file: ../../data/sogou_resource/dataset/财经.csv\n"
     ]
    }
   ],
   "source": [
    "# do extract\n",
    "dataset_dir = \"../../data/sogou_resource/dataset/\"\n",
    "train_dataset = os.path.join(dataset_dir,\"train.csv\")\n",
    "test_dataset = os.path.join(dataset_dir,\"test.csv\")\n",
    "if not os.path.exists(train_dataset):\n",
    "    print \"create file: \"+train_dataset\n",
    "    open(train_dataset,'w').write(\"type,content\\n\")\n",
    "if not os.path.exists(test_dataset):\n",
    "    print \"create file:\"+test_dataset\n",
    "    open(test_dataset,'w').write(\"type,content\\n\")\n",
    "    \n",
    "do_extract(dataset_dir,train_dataset,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "# 提取命名实体和名词、动词\n",
    "import requests\n",
    "from bosonnlp import BosonNLP\n",
    "\n",
    "Token = 'alM_aH0F.11177.NKoYB68fl19N'\n",
    "noun = ['n','nr','ns','nt','nz','nl','vd','vi','vl','nx']\n",
    "nlp = BosonNLP(Token)\n",
    "\n",
    "def extract_entity_and_nouns(content_raw):\n",
    "    result = nlp.ner(content_raw)[0]\n",
    "    words = result['word']\n",
    "    entities = result['entity']\n",
    "    tags = result['tag']\n",
    "    entity_list = [words[it[0]:it[1]] for it in entities]\n",
    "    con_entity = reduce(lambda x,y:x+y,entity_list)\n",
    "    con_noun = [it[0] for it in zip(words,tags) if it[1] in noun]\n",
    "    entity_noun_union = set(con_entity).union(set(con_noun))\n",
    "    \n",
    "    content = [word for word in [s.strip() for s in content_raw.split(' ')] if word in entity_noun_union]\n",
    "    return content\n",
    "\n",
    "def extract_nouns(content_raw):\n",
    "    result = nlp.tag(content_raw,space_mode=1)[0]\n",
    "    content = [d[0] for d in zip(result['word'],result['tag']) if d[1] in noun]\n",
    "    # 转成utf-8编码\n",
    "    content = map(lambda x:x.encode('utf-8'),content)\n",
    "    return content\n",
    "\n",
    "def do_extract_entity_and_nouns(content_file,newfile):\n",
    "    print \"do_extract_entity_and_nouns in \"+content_file\n",
    "    newf = open(newfile,'w')\n",
    "    istitle = True\n",
    "    with open(content_file) as f:\n",
    "        for line in f.readlines():\n",
    "            if istitle:\n",
    "                istitle = False\n",
    "                newf.write(line)\n",
    "                continue\n",
    "            type_content = line.split(\",\")\n",
    "            content_raw = type_content[1]\n",
    "            new_cont = extract_nouns(content_raw)\n",
    "            new_line = type_content[0]+','+' '.join(new_cont)+'\\n'\n",
    "            print new_line\n",
    "            newf.write(new_line)\n",
    "    newf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"../../data/sogou_resource/dataset/test.csv\"\n",
    "train_file = \"../../data/sogou_resource/dataset/train.csv\"\n",
    "new_test_file = \"../../data/sogou_resource/dataset/extract_test.csv\"\n",
    "new_train_file = \"../../data/sogou_resource/dataset/extract_train.csv\"\n",
    "do_extract_entity_and_nouns(test_file,new_test_file)\n",
    "do_extract_entity_and_nouns(train_file,new_train_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"清洗数据，除掉停用词，剔除坏样本\"\"\"\n",
    "def clean_stopwords(content_raw,stopwords_set):\n",
    "    content_list = [x for x in content_raw.split(\" \") if x!='']\n",
    "    common_set = set(content_list) & stopwords_set\n",
    "    new_content = filter(lambda x:x not in common_set,content_list)\n",
    "    return new_content\n",
    "\n",
    "def do_clean_stopwords(content_file,stopwords_file,newfile):\n",
    "    print \"clean stopwords in \"+content_file\n",
    "    stopwords = []\n",
    "    # 获取停用词\n",
    "    with open(stopwords_file) as fi:\n",
    "        for line in fi.readlines():\n",
    "            stopwords.append(line.strip())\n",
    "    newf = open(newfile,'w')\n",
    "    with open(content_file) as f:\n",
    "        for line in f.readlines():\n",
    "            type_content = line.split(split_type_content_mark)\n",
    "            content_raw = type_content[1]\n",
    "            new_cont = clean_stopwords(content_raw,set(stopwords))\n",
    "            new_line = type_content[0]+ split_type_content_mark +' '.join(new_cont)\n",
    "            newf.write(new_line)\n",
    "    newf.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean stopwords in ../../data/sogou_resource/dataset/extract_test.csv\n",
      "clean stopwords in ../../data/sogou_resource/dataset/extract_train.csv\n"
     ]
    }
   ],
   "source": [
    "test_file = \"../../data/sogou_resource/dataset/extract_test.csv\"\n",
    "train_file = \"../../data/sogou_resource/dataset/extract_train.csv\"\n",
    "new_test_file = \"../../data/sogou_resource/dataset/cleaned_extract_test.csv\"\n",
    "new_train_file = \"../../data/sogou_resource/dataset/cleaned_extract_train.csv\"\n",
    "stop_words_file = \"../../data/sogou_resource/dataset/news.stopwords.txt\"\n",
    "do_clean_stopwords(test_file,stop_words_file,new_test_file)\n",
    "do_clean_stopwords(train_file,stop_words_file,new_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"将文本句子化分割\"\"\"\n",
    "import zhon.hanzi\n",
    "import re\n",
    "\n",
    "\n",
    "def do_sentence_doc(doc_file,new_file):\n",
    "    print \"to sentence file:\",doc_file\n",
    "    newf = open(new_file,'w')\n",
    "    with open(doc_file) as f:\n",
    "        for line in f.readlines():\n",
    "            type_content = line.split(split_type_content_mark)\n",
    "            label = type_content[0]\n",
    "            content_raw = type_content[1]\n",
    "            new_content = re.sub(ur\"([%s])+\" %zhon.hanzi.non_stops,\" \",content_raw.decode('utf-8'))\n",
    "            new_content = re.sub(ur\"([%s])+\" %zhon.hanzi.stops,\"  \",content_raw.decode('utf-8'))\n",
    "            new_type_content = label+split_type_content_mark+new_content.encode('utf-8')\n",
    "            newf.write(new_type_content)\n",
    "    newf.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to sentence file: ../../data/sogou_resource/dataset/test_sentence_file.csv\n"
     ]
    }
   ],
   "source": [
    "test_file = base_data_dir + \"dataset/test_sentence_file.csv\"\n",
    "new_file = base_data_dir + \"dataset/sentence_file.csv\"\n",
    "do_sentence_doc(test_file,new_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
